# Website Traffic Log Analysis

This notebook guides you through a basic data engineering workflow to process and analyze website traffic logs.

## 1. Setup and Data Loading
First, we'll import the necessary libraries and load our log data.

```python
import pandas as pd
import io

# The log data is provided as a string in CSV format
csv_data = """
timestamp,ip_address,page_visited,status_code,user_agent
2023-10-26 08:00:01,192.168.1.1,/home,200,Chrome
2023-10-26 08:02:02,10.0.0.5,/products,200,Firefox
2023-10-26 08:04:01,172.16.0.10,/about,200,Safari
2023-10-26 08:06:03,192.168.1.2,/contact,200,Edge
2023-10-26 08:08:00,10.0.0.6,/blog/post-1,200,Chrome
2023-10-26 08:10:02,192.168.1.1,/products/item-a,200,Firefox
2023-10-26 08:12:01,10.0.0.5,/search?q=data,200,Safari
2023-10-26 08:14:03,172.16.0.10,/home,404,Edge
2023-10-26 08:16:00,192.168.1.2,/products,200,Chrome
2023-10-26 08:18:02,10.0.0.6,/about,200,Firefox
2023-10-26 08:20:01,192.168.1.1,/contact,200,Safari
2023-10-26 08:22:03,10.0.0.5,/blog/post-1,200,Edge
2023-10-26 08:24:00,172.16.0.10,/products/item-a,200,Chrome
2023-10-26 08:26:02,192.168.1.2,/search?q=data,500,Firefox
2023-10-26 08:28:01,10.0.0.6,/home,200,Safari
2023-10-26 08:30:03,192.168.1.1,/products,200,Edge
2023-10-26 08:32:00,10.0.0.5,/about,200,Chrome
2023-10-26 08:34:02,172.16.0.10,/contact,200,Firefox
2023-10-26 08:36:01,192.168.1.2,/blog/post-1,404,Safari
2023-10-26 08:38:03,10.0.0.6,/products/item-a,200,Edge
2023-10-26 08:40:00,192.168.1.1,/search?q=data,200,Chrome
2023-10-26 08:42:02,10.0.0.5,/home,200,Firefox
2023-10-26 08:44:01,172.16.0.10,/products,200,Safari
2023-10-26 08:46:03,192.168.1.2,/about,200,Edge
2023-10-26 08:48:00,10.0.0.6,/contact,200,Chrome
"""

df = pd.read_csv(io.StringIO(csv_data))

print("Original Data Head:")
print(df.head())
print("\nData Info:")
df.info()
```

## 2. Data Cleaning and Transformation
We'll convert the `timestamp` column to datetime objects and extract useful features like the hour of the day. We'll also ensure `status_code` is numeric. To ensure robustness, we encapsulate this in a function.

```python
def transform_data(input_df):
    df = input_df.copy()
    # Convert 'timestamp' to datetime objects
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # Extract hour of the day
    df['hour_of_day'] = df['timestamp'].dt.hour

    # Convert 'status_code' to integer type
    df['status_code'] = pd.to_numeric(df['status_code'])
    return df

# Apply transformation
df = transform_data(df)

print("\nTransformed Data Head:")
print(df.head())
print("\nTransformed Data Info:")
df.info()
```

## 3. Basic Data Analysis
Now, let's perform some basic analysis to understand our website traffic.

```python
def analyze_traffic(df):
    results = {}
    # 1. Number of unique visitors (based on IP address)
    results['unique_visitors'] = df['ip_address'].nunique()

    # 2. Most visited pages
    results['most_visited_pages'] = df['page_visited'].value_counts().head(5)

    # 3. Distribution of status codes
    results['status_code_distribution'] = df['status_code'].value_counts()

    # 4. Error rate (e.g., 4xx or 5xx status codes)
    error_requests = df[df['status_code'] >= 400]
    total_requests = len(df)
    results['error_rate'] = (len(error_requests) / total_requests) * 100 if total_requests > 0 else 0

    # 5. Traffic by hour of day
    results['traffic_by_hour'] = df['hour_of_day'].value_counts().sort_index()
    return results

analysis = analyze_traffic(df)

print(f"\nNumber of Unique Visitors: {analysis['unique_visitors']}")
print("\nMost Visited Pages:")
print(analysis['most_visited_pages'])
print(f"\nError Rate: {analysis['error_rate']:.2f}%")
```

## 4. Save Processed Data
Finally, we'll save our cleaned and transformed data to a new CSV file, simulating the 'Load' step in ETL.

```python
# Save the processed DataFrame to a new CSV file
output_filename = 'processed_website_logs.csv'
df.to_csv(output_filename, index=False)

print(f"\nProcessed data saved to {output_filename}")

# Display the first few rows of the saved CSV to confirm
processed_df = pd.read_csv(output_filename)
print("\nHead of saved processed data:")
print(processed_df.head())
```